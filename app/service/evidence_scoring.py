"""
å¢å¼ºç‰ˆè¯æ®è¯„åˆ†ä¸å¯è§†åŒ–æŠ¥å‘ŠæœåŠ¡
äº”ç»´è¯æ®æ¨¡å‹ + æ¯”ç‰¹çƒ­åŠ›å›¾ + æ³•å¾‹çº§åˆ†ææŠ¥å‘Š
"""

import numpy as np
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timezone
import hashlib
import json


@dataclass
class EvidenceDimension:
    """å•ä¸€è¯æ®ç»´åº¦è¯„åˆ†"""
    name: str  # ç»´åº¦åç§°
    score: float  # 0-100 åˆ†æ•°
    weight: float  # æƒé‡
    evidence_type: str  # è¯æ®ç±»å‹
    description: str  # è¯„åˆ†è¯´æ˜
    technical_details: Dict[str, Any] = field(default_factory=dict)  # æŠ€æœ¯ç»†èŠ‚


@dataclass
class FiveDimensionalScore:
    """äº”ç»´è¯æ®è¯„åˆ†æ¨¡å‹"""
    # äº”ä¸ªç»´åº¦
    fingerprint: EvidenceDimension  # æŒ‡çº¹ç½®ä¿¡åº¦
    temporal: EvidenceDimension       # æ—¶é—´ç½®ä¿¡åº¦ï¼ˆåŒºå—é“¾ï¼‰
    semantic: EvidenceDimension      # è¯­ä¹‰ç½®ä¿¡åº¦ï¼ˆå‘é‡ï¼‰
    robustness: EvidenceDimension    # é²æ£’æ€§ç½®ä¿¡åº¦ï¼ˆæŠ—æ”»å‡»ï¼‰
    provenance: EvidenceDimension     # æº¯æºç½®ä¿¡åº¦ï¼ˆåˆ›ä½œé“¾è·¯ï¼‰
    
    # èåˆåˆ†æ•°
    @property
    def total_score(self) -> float:
        """åŠ æƒèåˆæ€»åˆ†"""
        dims = [self.fingerprint, self.temporal, self.semantic, self.robustness, self.provenance]
        total_weight = sum(d.weight for d in dims)
        if total_weight == 0:
            return 0
        weighted_sum = sum(d.score * d.weight for d in dims)
        return round(weighted_sum / total_weight, 2)
    
    @property
    def confidence_level(self) -> str:
        """ç½®ä¿¡åº¦ç­‰çº§æ˜ å°„"""
        score = self.total_score
        if score >= 90:
            return "Açº§-ç¡®å®šæ€§è¯æ®"
        elif score >= 75:
            return "Bçº§-é«˜åº¦ç–‘ä¼¼"
        elif score >= 60:
            return "Cçº§-å¯èƒ½ç›¸å…³"
        elif score >= 40:
            return "Dçº§-å¼±å…³è”"
        else:
            return "Eçº§-ä¸ç›¸å…³"
    
    @property
    def legal_description(self) -> str:
        """æ³•å¾‹è¡¨è¿°"""
        level = self.confidence_level
        descriptions = {
            "Açº§-ç¡®å®šæ€§è¯æ®": "æŠ€æœ¯é‰´å®šç»“è®ºä¸º'ç¡®å®šæ€§æƒå±è¯æ®'ï¼ŒæŒ‡çº¹å®Œå…¨åŒ¹é…ä¸”è¯æ®é“¾å®Œæ•´ï¼Œå…·æœ‰æé«˜æ³•å¾‹å‚è€ƒä»·å€¼",
            "Bçº§-é«˜åº¦ç–‘ä¼¼": "æŠ€æœ¯æ¨å®šä¸º'é«˜åº¦ç–‘ä¼¼åŒæº'ï¼Œå¤šç»´åº¦è¯æ®ç›¸äº’å°è¯ï¼Œå»ºè®®ä½œä¸ºåˆæ­¥è¯æ®ä½¿ç”¨",
            "Cçº§-å¯èƒ½ç›¸å…³": "æŠ€æœ¯æ£€æµ‹æ˜¾ç¤º'å¯èƒ½å­˜åœ¨å…³è”'ï¼Œå•ä¸€è¯æ®åŒ¹é…ï¼Œéœ€è¡¥å……å…¶ä»–è¯æ®ä½è¯",
            "Dçº§-å¼±å…³è”": "æŠ€æœ¯æ£€æµ‹æ˜¾ç¤º'å¼±å…³è”æ€§'ï¼Œä»…ä¸ªåˆ«ç‰¹å¾ç›¸ä¼¼ï¼Œä¸æ„æˆæœ‰æ•ˆè¯æ®",
            "Eçº§-ä¸ç›¸å…³": "æŠ€æœ¯æ£€æµ‹æœªå‘ç°æœ‰æ•ˆåŒ¹é…ï¼Œæ— æ³•å»ºç«‹å…³è”æ€§"
        }
        return descriptions.get(level, "æœªçŸ¥")
    
    def to_dict(self) -> Dict:
        """åºåˆ—åŒ–ä¸ºå­—å…¸"""
        return {
            "total_score": self.total_score,
            "confidence_level": self.confidence_level,
            "legal_description": self.legal_description,
            "dimensions": {
                "fingerprint": {
                    "name": self.fingerprint.name,
                    "score": self.fingerprint.score,
                    "weight": self.fingerprint.weight,
                    "description": self.fingerprint.description,
                    "technical_details": self.fingerprint.technical_details
                },
                "temporal": {
                    "name": self.temporal.name,
                    "score": self.temporal.score,
                    "weight": self.temporal.weight,
                    "description": self.temporal.description,
                    "technical_details": self.temporal.technical_details
                },
                "semantic": {
                    "name": self.semantic.name,
                    "score": self.semantic.score,
                    "weight": self.semantic.weight,
                    "description": self.semantic.description,
                    "technical_details": self.semantic.technical_details
                },
                "robustness": {
                    "name": self.robustness.name,
                    "score": self.robustness.score,
                    "weight": self.robustness.weight,
                    "description": self.robustness.description,
                    "technical_details": self.robustness.technical_details
                },
                "provenance": {
                    "name": self.provenance.name,
                    "score": self.provenance.score,
                    "weight": self.provenance.weight,
                    "description": self.provenance.description,
                    "technical_details": self.provenance.technical_details
                }
            }
        }


class EvidenceScorer:
    """è¯æ®è¯„åˆ†è®¡ç®—å™¨"""
    
    @staticmethod
    def calculate_fingerprint_score(
        similarity: float,
        fingerprint_strength: int,
        extraction_confidence: float,
        fragment_match_rate: float
    ) -> EvidenceDimension:
        """
        è®¡ç®—æŒ‡çº¹ç½®ä¿¡åº¦
        
        Args:
            similarity: æŒ‡çº¹ç›¸ä¼¼åº¦ (0-100)
            fingerprint_strength: æŒ‡çº¹å¼ºåº¦ (0-256)
            extraction_confidence: æå–ç½®ä¿¡åº¦ (0-1)
            fragment_match_rate: ç‰‡æ®µåŒ¹é…ç‡ (0-100)
        """
        # å­ç»´åº¦è¯„åˆ†
        sim_score = min(100, similarity)  # ç›¸ä¼¼åº¦å æ¯”40%
        strength_score = min(100, fingerprint_strength / 256 * 100)  # å¼ºåº¦å æ¯”25%
        extract_score = extraction_confidence * 100  # æå–ç½®ä¿¡åº¦å æ¯”20%
        fragment_score = fragment_match_rate  # ç‰‡æ®µåŒ¹é…å æ¯”15%
        
        # åŠ æƒè®¡ç®—
        score = (
            sim_score * 0.40 +
            strength_score * 0.25 +
            extract_score * 0.20 +
            fragment_score * 0.15
        )
        
        # æŠ€æœ¯ç»†èŠ‚
        tech_details = {
            "similarity_contribution": round(sim_score * 0.40, 2),
            "strength_contribution": round(strength_score * 0.25, 2),
            "extraction_contribution": round(extract_score * 0.20, 2),
            "fragment_contribution": round(fragment_score * 0.15, 2),
            "raw_similarity": similarity,
            "raw_strength": fingerprint_strength,
            "extraction_confidence": extraction_confidence,
            "fragment_match_rate": fragment_match_rate,
            "algorithm": "DCTé¢‘åŸŸæŒ‡çº¹æå– + æ±‰æ˜è·ç¦»æ¯”å¯¹",
            "bit_length": 256,
            "dimensionality": "é¢‘åŸŸDCTç³»æ•°"
        }
        
        return EvidenceDimension(
            name="æ•°å­—æŒ‡çº¹ç½®ä¿¡åº¦",
            score=round(score, 2),
            weight=0.40,  # æŒ‡çº¹æƒé‡æœ€é«˜
            evidence_type="ç¡®å®šæ€§è¯æ®",
            description=f"æŒ‡çº¹ç›¸ä¼¼åº¦{similarity:.1f}%ï¼Œå¼ºåº¦{fingerprint_strength}/256ï¼Œæå–ç½®ä¿¡åº¦{extraction_confidence*100:.1f}%",
            technical_details=tech_details
        )
    
    @staticmethod
    def calculate_temporal_score(
        has_blockchain_record: bool,
        creation_timestamp: Optional[int],
        detection_timestamp: int,
        time_consistency: bool
    ) -> EvidenceDimension:
        """
        è®¡ç®—æ—¶é—´ç½®ä¿¡åº¦ï¼ˆåŒºå—é“¾å­˜è¯ï¼‰
        
        Args:
            has_blockchain_record: æ˜¯å¦æœ‰åŒºå—é“¾è®°å½•
            creation_timestamp: åˆ›ä½œæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
            detection_timestamp: æ£€æµ‹æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
            time_consistency: æ—¶é—´é€»è¾‘æ˜¯å¦ä¸€è‡´ï¼ˆåˆ›ä½œ<æ£€æµ‹ï¼‰
        """
        if not has_blockchain_record:
            return EvidenceDimension(
                name="æ—¶é—´é“¾ç½®ä¿¡åº¦",
                score=0,
                weight=0.20,
                evidence_type="æ—¶é—´è¯æ®",
                description="æ— åŒºå—é“¾å­˜è¯è®°å½•",
                technical_details={"reason": "æœªä¸Šé“¾"}
            )
        
        # åŸºç¡€åˆ†ï¼ˆæœ‰åŒºå—é“¾å°±æœ‰60åˆ†ï¼‰
        base_score = 60
        
        # æ—¶é—´åˆç†æ€§åŠ åˆ†
        consistency_bonus = 20 if time_consistency else 0
        
        # æ—¶é—´è·¨åº¦åˆç†æ€§ï¼ˆåˆ›ä½œè·ä»Šè¶Šä¹…ï¼Œå¯ä¿¡åº¦è¶Šé«˜ï¼Œé˜²æ­¢"äº‹åä¼ªé€ "ï¼‰
        time_span_bonus = 0
        if creation_timestamp:
            time_span_days = (detection_timestamp - creation_timestamp) / 86400
            if time_span_days > 30:  # åˆ›ä½œæ—¶é—´æ—©äºæ£€æµ‹æ—¶é—´30å¤©ä»¥ä¸Š
                time_span_bonus = 20
            elif time_span_days > 7:
                time_span_bonus = 10
        
        score = min(100, base_score + consistency_bonus + time_span_bonus)
        
        tech_details = {
            "blockchain_verified": has_blockchain_record,
            "creation_timestamp": creation_timestamp,
            "detection_timestamp": detection_timestamp,
            "time_consistency": time_consistency,
            "consistency_bonus": consistency_bonus,
            "time_span_bonus": time_span_bonus,
            "blockchain_type": "è”ç›Ÿé“¾/å…¬é“¾å­˜è¯",
            "immutable": True,
            "timestamp_format": "Unix timestamp (seconds)"
        }
        
        desc = f"åŒºå—é“¾å­˜è¯å·²ç¡®è®¤ï¼Œæ—¶é—´é€»è¾‘{'ä¸€è‡´' if time_consistency else 'å­˜ç–‘'}"
        if creation_timestamp:
            desc += f"ï¼Œåˆ›ä½œæ—¶é—´ï¼š{datetime.fromtimestamp(creation_timestamp).strftime('%Y-%m-%d')}"
        
        return EvidenceDimension(
            name="æ—¶é—´é“¾ç½®ä¿¡åº¦",
            score=score,
            weight=0.20,
            evidence_type="æ—¶é—´è¯æ®",
            description=desc,
            technical_details=tech_details
        )
    
    @staticmethod
    def calculate_semantic_score(
        faiss_similarity: float,
        vector_match_count: int,
        top_k_confidence: float
    ) -> EvidenceDimension:
        """
        è®¡ç®—è¯­ä¹‰ç½®ä¿¡åº¦ï¼ˆæ·±åº¦å‘é‡ï¼‰
        
        Args:
            faiss_similarity: FAISSä½™å¼¦ç›¸ä¼¼åº¦ (0-1)
            vector_match_count: åŒ¹é…çš„å‘é‡æ•°é‡
            top_k_confidence: Top-Kç½®ä¿¡åº¦
        """
        # åŸºç¡€åˆ†æ¥è‡ªç›¸ä¼¼åº¦
        sim_score = faiss_similarity * 100
        
        # åŒ¹é…æ•°é‡åŠ åˆ†
        count_bonus = min(20, vector_match_count * 5)
        
        # Top-Kç½®ä¿¡åº¦åŠ æƒ
        top_k_weight = top_k_confidence * 0.1
        
        score = min(100, sim_score + count_bonus + top_k_weight)
        
        tech_details = {
            "faiss_similarity": faiss_similarity,
            "vector_match_count": vector_match_count,
            "top_k_confidence": top_k_confidence,
            "algorithm": "FAISS-IVF + CLIPåµŒå…¥",
            "embedding_model": "CLIP-ResNet50",
            "vector_dimension": 512,
            "metric": "ä½™å¼¦ç›¸ä¼¼åº¦",
            "similarity_contribution": round(sim_score, 2),
            "count_bonus": count_bonus
        }
        
        return EvidenceDimension(
            name="è¯­ä¹‰ç½®ä¿¡åº¦",
            score=round(score, 2),
            weight=0.15,
            evidence_type="æ¨å®šè¯æ®",
            description=f"æ·±åº¦å‘é‡ç›¸ä¼¼åº¦{faiss_similarity*100:.1f}%ï¼ŒåŒ¹é…å‘é‡æ•°{vector_match_count}",
            technical_details=tech_details
        )
    
    @staticmethod
    def calculate_robustness_score(
        psnr_value: float,
        compression_resistance: float,
        crop_resistance: float,
        filter_resistance: float
    ) -> EvidenceDimension:
        """
        è®¡ç®—é²æ£’æ€§ç½®ä¿¡åº¦ï¼ˆæŠ—æ”»å‡»æµ‹è¯•ï¼‰
        
        Args:
            psnr_value: PSNRå€¼ï¼ˆè¶Šé«˜è¶Šä¸æ˜“å¯Ÿè§‰ï¼‰
            compression_resistance: å‹ç¼©æŠ—æ€§ (0-1)
            crop_resistance: è£å‰ªæŠ—æ€§ (0-1)
            filter_resistance: æ»¤é•œæŠ—æ€§ (0-1)
        """
        # PSNRè¯„åˆ†ï¼ˆç†æƒ³çš„PSNRåœ¨35-45ä¹‹é—´ï¼‰
        psnr_score = max(0, min(100, (psnr_value - 20) / 30 * 100))
        
        # ç»¼åˆæŠ—æ€§
        resistance_avg = (compression_resistance + crop_resistance + filter_resistance) / 3
        resistance_score = resistance_avg * 100
        
        # åŠ æƒ
        score = psnr_score * 0.3 + resistance_score * 0.7
        
        tech_details = {
            "psnr": psnr_value,
            "psnr_score": round(psnr_score, 2),
            "compression_resistance": compression_resistance,
            "crop_resistance": crop_resistance,
            "filter_resistance": filter_resistance,
            "resistance_avg": round(resistance_avg, 4),
            "resistance_score": round(resistance_score, 2),
            "attacks_tested": ["JPEGå‹ç¼©ï¼ˆè´¨é‡80%ï¼‰", "ä¸­å¿ƒè£å‰ªï¼ˆä¿ç•™60%ï¼‰", "é«˜æ–¯æ¨¡ç³Šï¼ˆÏƒ=1.0ï¼‰"],
            "watermark_strength": "å¯è°ƒèŠ‚0.05-0.3",
            "dct_coefficients": "ä¸­é¢‘å¸¦åµŒå…¥"
        }
        
        return EvidenceDimension(
            name="é²æ£’æ€§ç½®ä¿¡åº¦",
            score=round(score, 2),
            weight=0.15,
            evidence_type="é²æ£’æ€§è¯æ®",
            description=f"PSNR={psnr_value:.1f}dBï¼Œç»¼åˆæŠ—æ€§{resistance_avg*100:.1f}%",
            technical_details=tech_details
        )
    
    @staticmethod
    def calculate_provenance_score(
        author_verified: bool,
        creation_chain_complete: bool,
        historical_consistency: bool,
        cross_platform_verified: bool
    ) -> EvidenceDimension:
        """
        è®¡ç®—æº¯æºç½®ä¿¡åº¦ï¼ˆåˆ›ä½œé“¾è·¯å®Œæ•´åº¦ï¼‰
        
        Args:
            author_verified: ä½œè€…èº«ä»½å·²éªŒè¯
            creation_chain_complete: åˆ›ä½œé“¾è·¯å®Œæ•´
            historical_consistency: å†å²è®°å½•ä¸€è‡´
            cross_platform_verified: è·¨å¹³å°éªŒè¯é€šè¿‡
        """
        score = 0
        details = {}
        
        if author_verified:
            score += 30
            details["author_verified"] = {"points": 30, "status": "å·²éªŒè¯"}
        else:
            details["author_verified"] = {"points": 0, "status": "æœªéªŒè¯"}
        
        if creation_chain_complete:
            score += 25
            details["creation_chain"] = {"points": 25, "status": "å®Œæ•´"}
        else:
            details["creation_chain"] = {"points": 0, "status": "ç¼ºå¤±"}
        
        if historical_consistency:
            score += 25
            details["historical_consistency"] = {"points": 25, "status": "ä¸€è‡´"}
        else:
            details["historical_consistency"] = {"points": 0, "status": "å­˜ç–‘"}
        
        if cross_platform_verified:
            score += 20
            details["cross_platform"] = {"points": 20, "status": "å·²éªŒè¯"}
        else:
            details["cross_platform"] = {"points": 0, "status": "æœªéªŒè¯"}
        
        tech_details = {
            **details,
            "verification_methods": [
                "å¹³å°è´¦å·å®åè®¤è¯",
                "åˆ›ä½œå·¥å…·æ•°å­—ç­¾å",
                "å†å²ä½œå“é£æ ¼ä¸€è‡´æ€§åˆ†æ",
                "ç¤¾äº¤åª’ä½“äº¤å‰éªŒè¯"
            ]
        }
        
        desc_parts = []
        if author_verified:
            desc_parts.append("ä½œè€…å·²éªŒè¯")
        if creation_chain_complete:
            desc_parts.append("é“¾è·¯å®Œæ•´")
        if historical_consistency:
            desc_parts.append("å†å²ä¸€è‡´")
        
        description = "ï¼Œ".join(desc_parts) if desc_parts else "æº¯æºä¿¡æ¯ä¸è¶³"
        
        return EvidenceDimension(
            name="æº¯æºç½®ä¿¡åº¦",
            score=score,
            weight=0.10,
            evidence_type="æº¯æºè¯æ®",
            description=description,
            technical_details=tech_details
        )
    
    @classmethod
    def calculate_all_scores(
        cls,
        detection_result: Dict,
        blockchain_data: Optional[Dict] = None
    ) -> FiveDimensionalScore:
        """
        è®¡ç®—å®Œæ•´äº”ç»´è¯„åˆ†
        """
        # æå–åŸºç¡€æ•°æ®
        match_summary = detection_result.get("match_summary", {})
        best_match = detection_result.get("best_match", {})
        fingerprint_detail = detection_result.get("extracted_fingerprint_detail", {}) or {}
        
        # 1. æŒ‡çº¹ç½®ä¿¡åº¦
        fingerprint_dim = cls.calculate_fingerprint_score(
            similarity=best_match.get("similarity", 0) if best_match else 0,
            fingerprint_strength=fingerprint_detail.get("strength_score", 0),
            extraction_confidence=0.85 if detection_result.get("has_watermark") else 0.3,
            fragment_match_rate=best_match.get("fingerprint_fragment_match", 0) if best_match else 0
        )
        
        # 2. æ—¶é—´ç½®ä¿¡åº¦
        has_blockchain = bool(blockchain_data and blockchain_data.get("tx_hash"))
        creation_ts = None
        if blockchain_data and blockchain_data.get("timestamp"):
            try:
                creation_ts = int(blockchain_data["timestamp"])
            except:
                pass
        
        temporal_dim = cls.calculate_temporal_score(
            has_blockchain_record=has_blockchain,
            creation_timestamp=creation_ts,
            detection_timestamp=int(datetime.now(timezone.utc).timestamp()),
            time_consistency=True  # ç®€åŒ–ï¼Œå®é™…éœ€è¦æ¯”è¾ƒæ—¶é—´
        )
        
        # 3. è¯­ä¹‰ç½®ä¿¡åº¦
        faiss_match = detection_result.get("deep_learning_match", {})
        semantic_dim = cls.calculate_semantic_score(
            faiss_similarity=faiss_match.get("similarity", 0) / 100 if faiss_match else 0,
            vector_match_count=1 if faiss_match else 0,
            top_k_confidence=0.8 if faiss_match else 0
        )
        
        # 4. é²æ£’æ€§ç½®ä¿¡åº¦ï¼ˆä»ç»“æœä¸­æå–æˆ–é»˜è®¤å€¼ï¼‰
        robustness_dim = cls.calculate_robustness_score(
            psnr_value=detection_result.get("psnr", 40.0),
            compression_resistance=0.85,
            crop_resistance=0.75,
            filter_resistance=0.70
        )
        
        # 5. æº¯æºç½®ä¿¡åº¦
        watermark_details = detection_result.get("watermark_details", {})
        provenance_dim = cls.calculate_provenance_score(
            author_verified=bool(watermark_details and watermark_details.get("author_name")),
            creation_chain_complete=bool(blockchain_data),
            historical_consistency=bool(best_match),
            cross_platform_verified=False  # éœ€è¦é¢å¤–éªŒè¯
        )
        
        return FiveDimensionalScore(
            fingerprint=fingerprint_dim,
            temporal=temporal_dim,
            semantic=semantic_dim,
            robustness=robustness_dim,
            provenance=provenance_dim
        )


class FingerprintVisualizer:
    """æŒ‡çº¹å¯è§†åŒ–ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_bit_heatmap(
        fingerprint1: str,
        fingerprint2: str,
        size: int = 8
    ) -> List[List[Dict]]:
        """
        ç”Ÿæˆæ¯”ç‰¹çº§çƒ­åŠ›å›¾æ•°æ®ï¼ˆ8x8ç½‘æ ¼ï¼Œæ¯æ ¼ä»£è¡¨64æ¯”ç‰¹ï¼‰
        
        Returns:
            8x8çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            - cell_index: æ ¼å­ç´¢å¼•(0-63)
            - match_rate: åŒ¹é…ç‡(0-100)
            - bits: è¯¥æ ¼å­çš„æ¯”ç‰¹å­—ç¬¦ä¸²
            - color_intensity: é¢œè‰²å¼ºåº¦(0-1)
        """
        if not fingerprint1 or not fingerprint2:
            return [[{"cell_index": i*size+j, "match_rate": 0, "bits": "", "color_intensity": 0} 
                     for j in range(size)] for i in range(size)]
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(fingerprint1), len(fingerprint2))
        fp1 = fingerprint1[:min_len]
        fp2 = fingerprint2[:min_len]
        
        # è®¡ç®—æ¯ä¸ªæ ¼å­çš„åŒ¹é…ç‡
        heatmap = []
        bits_per_cell = min_len // (size * size)
        if bits_per_cell == 0:
            bits_per_cell = 1
        
        for row in range(size):
            heatmap_row = []
            for col in range(size):
                cell_idx = row * size + col
                start_bit = cell_idx * bits_per_cell
                end_bit = min(start_bit + bits_per_cell, min_len)
                
                if start_bit >= min_len:
                    match_rate = 0
                    bits = ""
                else:
                    # è®¡ç®—è¯¥ç‰‡æ®µçš„åŒ¹é…ç‡
                    segment1 = fp1[start_bit:end_bit]
                    segment2 = fp2[start_bit:end_bit]
                    matches = sum(1 for a, b in zip(segment1, segment2) if a == b)
                    match_rate = (matches / len(segment1)) * 100 if segment1 else 0
                    bits = segment1[:8]  # åªæ˜¾ç¤ºå‰8ä½ä½œä¸ºç¤ºä¾‹
                
                heatmap_row.append({
                    "cell_index": cell_idx,
                    "match_rate": round(match_rate, 1),
                    "bits": bits,
                    "color_intensity": match_rate / 100,
                    "row": row,
                    "col": col
                })
            heatmap.append(heatmap_row)
        
        return heatmap
    
    @staticmethod
    def generate_radar_chart_data(
        five_dim_score: FiveDimensionalScore
    ) -> Dict:
        """
        ç”Ÿæˆé›·è¾¾å›¾æ•°æ®
        """
        return {
            "labels": ["æŒ‡çº¹ç½®ä¿¡åº¦", "æ—¶é—´é“¾", "è¯­ä¹‰ç›¸ä¼¼", "é²æ£’æ€§", "æº¯æºå®Œæ•´"],
            "datasets": [{
                "label": "è¯æ®å¼ºåº¦",
                "data": [
                    five_dim_score.fingerprint.score,
                    five_dim_score.temporal.score,
                    five_dim_score.semantic.score,
                    five_dim_score.robustness.score,
                    five_dim_score.provenance.score
                ],
                "backgroundColor": "rgba(99, 102, 241, 0.2)",
                "borderColor": "rgba(99, 102, 241, 1)",
                "borderWidth": 2
            }],
            "weights": [0.40, 0.20, 0.15, 0.15, 0.10],
            "total_score": five_dim_score.total_score,
            "level": five_dim_score.confidence_level
        }
    
    @staticmethod
    def generate_evidence_timeline(
        detection_result: Dict,
        blockchain_data: Optional[Dict] = None
    ) -> List[Dict]:
        """
        ç”Ÿæˆè¯æ®é“¾æ—¶é—´çº¿
        å¢å¼ºç‰ˆï¼šåŒ…å«åˆ›ä½œã€æŒ‡çº¹åµŒå…¥ã€å€™é€‰åŒ¹é…ç¡®æƒã€åŒºå—é“¾å­˜è¯ã€æ£€æµ‹ç­‰å¤šç±»äº‹ä»¶
        """
        timeline = []
        seen_ts = set()  # é¿å…åŒä¸€æ—¶é—´æˆ³é‡å¤

        def _safe_ts(raw) -> Optional[int]:
            """å®‰å…¨è½¬æ¢æ—¶é—´æˆ³ï¼Œæ”¯æŒæ•´æ•°/æµ®ç‚¹/ISOå­—ç¬¦ä¸²"""
            if raw is None:
                return None
            try:
                return int(float(raw))
            except (ValueError, TypeError):
                pass
            # å°è¯• ISO æ ¼å¼
            if isinstance(raw, str):
                try:
                    return int(datetime.fromisoformat(raw.replace('Z', '+00:00')).timestamp())
                except Exception:
                    pass
            return None

        def _add(event: str, ts: int, description: str, evidence_type: str, icon: str):
            if ts in seen_ts:
                return
            seen_ts.add(ts)
            timeline.append({
                "event": event,
                "timestamp": ts,
                "time_str": datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'),
                "description": description,
                "evidence_type": evidence_type,
                "icon": icon,
            })

        # 1. åˆ›ä½œæ—¶é—´ï¼ˆæ¥è‡ªæ°´å°å…ƒæ•°æ®ï¼‰
        watermark_details = detection_result.get("watermark_details", {})
        creation_ts = _safe_ts(watermark_details.get("timestamp"))
        if creation_ts:
            _add(
                "ä½œå“åˆ›ä½œ",
                creation_ts,
                f"ä½œè€…ï¼š{watermark_details.get('author_name', 'æœªçŸ¥')}",
                "åˆ›ä½œèµ·ç‚¹",
                "âœï¸",
            )

        # 2. æœ€ä½³åŒ¹é…èµ„äº§çš„æŒ‡çº¹ç¡®æƒæ—¶é—´
        best_match = detection_result.get("best_match") or {}
        bm_ts = _safe_ts(best_match.get("timestamp") or best_match.get("creation_time"))
        if bm_ts:
            _add(
                "æŒ‡çº¹ç¡®æƒ(æœ€ä½³åŒ¹é…)",
                bm_ts,
                f"åŒ¹é…ä½œè€…ï¼š{best_match.get('author_name', best_match.get('author', 'æœªçŸ¥'))}ï¼Œ"
                f"ç›¸ä¼¼åº¦ï¼š{best_match.get('similarity', 0)}%",
                "æŒ‡çº¹ç¡®æƒ",
                "ğŸ”",
            )

        # 3. å€™é€‰åŒ¹é…åˆ—è¡¨ä¸­ä¸åŒæŒ‡çº¹çš„ç¡®æƒæ—¶æœŸï¼ˆæœ€å¤šå– Top3 ä»¥å…è¿‡äºæ‹¥æŒ¤ï¼‰
        candidates = detection_result.get("match_candidates", [])
        for i, cand in enumerate(candidates[:3]):
            cand_ts = _safe_ts(cand.get("timestamp") or cand.get("creation_time"))
            if cand_ts:
                _add(
                    f"å€™é€‰æŒ‡çº¹#{i+1}",
                    cand_ts,
                    f"ä½œè€…ï¼š{cand.get('author', cand.get('author_name', 'æœªçŸ¥'))}ï¼Œ"
                    f"ç›¸ä¼¼åº¦ï¼š{cand.get('similarity', 0)}%",
                    "å€™é€‰ç¡®æƒ",
                    "ğŸ“Œ",
                )

        # 4. æŒ‡çº¹åµŒå…¥æ—¶é—´ï¼ˆæ¥è‡ªæå–åˆ°çš„æŒ‡çº¹è¯¦æƒ…ï¼‰
        fp_detail = detection_result.get("extracted_fingerprint_detail") or (
            detection_result.get("extracted_fingerprint") if isinstance(detection_result.get("extracted_fingerprint"), dict) else {}
        )
        embed_ts = _safe_ts(fp_detail.get("embed_timestamp") or fp_detail.get("timestamp"))
        if embed_ts and embed_ts != creation_ts:
            _add(
                "æŒ‡çº¹åµŒå…¥",
                embed_ts,
                f"æŒ‡çº¹å¼ºåº¦ï¼š{fp_detail.get('strength_score', 'N/A')}/256",
                "æŠ€æœ¯æº¯æº",
                "ğŸ§¬",
            )

        # 5. åŒºå—é“¾å­˜è¯
        if blockchain_data and blockchain_data.get("timestamp"):
            bc_ts = _safe_ts(blockchain_data["timestamp"])
            if bc_ts:
                _add(
                    "åŒºå—é“¾å­˜è¯",
                    bc_ts,
                    f"äº¤æ˜“å“ˆå¸Œï¼š{blockchain_data.get('tx_hash', 'N/A')[:16]}...",
                    "ä¸å¯ç¯¡æ”¹è¯æ®",
                    "ğŸ”—",
                )

        # å¦‚æœ best_match ä¸­æœ‰ tx_hashï¼Œä¹Ÿæ·»åŠ å…¶ä¸Šé“¾æ—¶é—´
        bm_tx_ts = _safe_ts(best_match.get("blockchain_timestamp") or best_match.get("tx_timestamp"))
        if bm_tx_ts:
            _add(
                "åŒ¹é…èµ„äº§ä¸Šé“¾",
                bm_tx_ts,
                f"TxHashï¼š{best_match.get('tx_hash', 'N/A')[:16]}...",
                "é“¾ä¸Šå­˜è¯",
                "â›“ï¸",
            )

        # 6. æŠ€æœ¯æ£€æµ‹æ—¶é—´
        now_ts = int(datetime.now(timezone.utc).timestamp())
        _add(
            "æŠ€æœ¯æ£€æµ‹",
            now_ts,
            "æ•°å­—æŒ‡çº¹æå–ä¸æ¯”å¯¹åˆ†æ",
            "æŠ€æœ¯é‰´å®š",
            "ğŸ”",
        )

        # æŒ‰æ—¶é—´æ’åº
        timeline.sort(key=lambda x: x["timestamp"])

        # æ·»åŠ æ—¶é—´é—´éš”è®¡ç®—
        for i in range(1, len(timeline)):
            prev = timeline[i - 1]
            curr = timeline[i]
            interval_hours = (curr["timestamp"] - prev["timestamp"]) / 3600
            if interval_hours < 1:
                curr["interval_from_prev"] = f"{interval_hours * 60:.0f}åˆ†é’Ÿ"
            elif interval_hours < 24:
                curr["interval_from_prev"] = f"{interval_hours:.1f}å°æ—¶"
            else:
                curr["interval_from_prev"] = f"{interval_hours / 24:.1f}å¤©"

        return timeline


# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'EvidenceDimension',
    'FiveDimensionalScore', 
    'EvidenceScorer',
    'FingerprintVisualizer'
]
